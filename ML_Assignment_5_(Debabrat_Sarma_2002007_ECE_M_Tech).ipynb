{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment_5_(Debabrat_Sarma_2002007_ECE_M.Tech).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrbF6Z7+m/Pn5dx/tVFD31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debabrat1/Machine_Learning_Codes/blob/main/ML_Assignment_5_(Debabrat_Sarma_2002007_ECE_M_Tech).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJM2PzM8dvB9"
      },
      "source": [
        "a) Implement the linear regression using the Ordinary Least Squares (OLS) Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ0ZnQtleEKe"
      },
      "source": [
        "# Making imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (12.0, 9.0)\n",
        "\n",
        "# Preprocessing Input data\n",
        "data = pd.read_csv('slr06.csv')\n",
        "X = data.iloc[:, 0]\n",
        "Y = data.iloc[:, 1]\n",
        "plt.scatter(X, Y)\n",
        "\n",
        "#Building the model\n",
        "X_mean = np.mean(X)\n",
        "Y_mean = np.mean(Y)\n",
        "\n",
        "num = 0\n",
        "den = 0\n",
        "for i in range(len(X)):\n",
        "    num += (X[i] - X_mean)*(Y[i] - Y_mean)\n",
        "    den += (X[i] - X_mean)**2\n",
        "m = num / den\n",
        "c = Y_mean - m*X_mean\n",
        "\n",
        "print (m, c)\n",
        "\n",
        "# Making predictions\n",
        "Y_pred = m*X + c\n",
        "\n",
        "plt.scatter(X, Y) # actual\n",
        "# plt.scatter(X, Y_pred, color='red')\n",
        "plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red') # predicted\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tnJ3kdGeKGj"
      },
      "source": [
        "(b) Gradiant Descent Algoritm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLp6ZaNlez-W"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "dataset = pd.read_csv(r'slr06.csv')\n",
        "dataset.head()\n",
        "X = dataset['X'].values\n",
        "y = dataset['Y'].values\n",
        "#Visualizing the data\n",
        "title='Linear Regression on <Dataset>'\n",
        "x_axis_label = 'X-value < The corresponding attribute of X in dataset >'\n",
        "y_axis_label = 'y-value < The corresponding attribute of X in dataset >'\n",
        "title='Linear Regression on Auto Insurance Sweden Dataset'\n",
        "x_axis_label = \"Total Payment\"\n",
        "y_axis_label = \"Number of Claims\"\n",
        "\n",
        "plt.scatter(X,y)\n",
        "plt.title(title)\n",
        "plt.xlabel(x_axis_label)\n",
        "plt.ylabel(y_axis_label)\n",
        "plt.show()\n",
        "\n",
        "# Splitting the data into training set, validation set and  test set\n",
        "X_train,X_val,X_test = np.split(X,indices_or_sections = [int(.6*len(X)), int(.8*len(X))])\n",
        "y_train,y_val,y_test = np.split(y,indices_or_sections = [int(.6*len(y)), int(.8*len(y))])\n",
        "print(\"Train: \",X_train,\"\\nValidation: \", X_val,\"\\nTest: \", X_test)\n",
        "\n",
        "#Functions for calculation of gradient descent Function to calculate mean square error\n",
        "def mean_square_error(slope,intercept,x_data,y_data):\n",
        "    n = len(x_data)\n",
        "    sum_of_squares = 0\n",
        "    for i in range(n):\n",
        "        y_pred = x_data[i]*slope+intercept\n",
        "        y_actual = y_data[i]\n",
        "        sum_of_squares += (y_actual - y_pred)**2  \n",
        "        \n",
        "    mean_square_error = math.sqrt(sum_of_squares/n)\n",
        "    return mean_square_error\n",
        "#Function to calculate Gradient of slope at each intermediate iteration\n",
        "def gradient_slope(slope,intercept,x_data,y_data):\n",
        "    n = len(x_data)\n",
        "    sum_of_gradients = 0\n",
        "    for i in range(n):\n",
        "        y_pred = x_data[i]*slope + intercept\n",
        "        y_actual = y_data[i]\n",
        "        sum_of_gradients += x_data[i]*(y_actual-y_pred)\n",
        "    sum_of_gradients = -1*sum_of_gradients\n",
        "    gradient_slope = sum_of_gradients*(2/n)\n",
        "    return gradient_slope\n",
        "#Function to calculate Gradient of intercept at each intermediate iteration\n",
        "def gradient_intercept(slope,intercept,x_data,y_data):\n",
        "    n = len(x_data)\n",
        "    sum_of_gradients = 0\n",
        "    for i in range(n):\n",
        "        y_pred = x_data[i]*slope +intercept\n",
        "        y_actual = y_data[i]\n",
        "        sum_of_gradients += -1 *(y_actual-y_pred)\n",
        "    sum_of_gradients = -1*sum_of_gradients\n",
        "    gradient_intercept = sum_of_gradients*(2/n)\n",
        "    return gradient_intercept\n",
        "#Gradient Decent function performances gradient descent optimization for the given epochs withe the given learning rate.\n",
        "slope_history = []\n",
        "intercept_history = []\n",
        "loss_history = []\n",
        "def gradient_decent(x_data,y_data,epochs,learning_rate,initial_slope,initial_intercept):\n",
        "    slope = initial_slope\n",
        "    intercept = initial_intercept\n",
        "    for epoch in range(epochs):\n",
        "        slope  = slope - learning_rate*(gradient_slope(slope,intercept,x_data,y_data))\n",
        "        intercept = intercept - learning_rate*(gradient_intercept(slope,intercept,x_data,y_data))\n",
        "        loss = mean_square_error(slope,intercept,x_data,y_data)\n",
        "        loss_history.append(loss)\n",
        "        slope_history.append(slope)\n",
        "        intercept_history.append(intercept)\n",
        "    return (slope,intercept)\n",
        "\n",
        "#initial all the hyperparameters and run to gradient decent and make changes to hyperparmeters until the funtion reduces its loss.\n",
        "epochs = 500\n",
        "learning_rate = 0.00001\n",
        "initial_slope = 0\n",
        "initial_intercept = 0\n",
        "\n",
        "(slope,intercept) = gradient_decent(X_train,y_train,epochs = epochs,learning_rate=learning_rate,initial_slope = initial_slope,initial_intercept =initial_intercept)\n",
        "\n",
        "# The plot to inspect the decrease in loss\n",
        "plt.title(\"Loss Function\")\n",
        "plt.plot(loss_history)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# The plot to inspect the variation in slope and intercept\n",
        "plt.plot(slope_history)\n",
        "plt.plot(intercept_history)\n",
        "plt.show()\n",
        "\n",
        "m = slope\n",
        "c = intercept\n",
        "# Predicting the Results\n",
        "y_pred = X_test*m + c\n",
        "# Visualizing the Results\n",
        "plt.scatter(X_test,y_test,c='red')\n",
        "plt.plot(X_test,y_pred)\n",
        "\n",
        "plt.title(title)\n",
        "plt.xlabel(x_axis_label)\n",
        "plt.ylabel(y_axis_label)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}